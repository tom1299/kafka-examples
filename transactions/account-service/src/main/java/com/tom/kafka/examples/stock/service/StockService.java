/*
 * This Java source file was generated by the Gradle 'init' task.
 */
package com.tom.kafka.examples.stock.service;

import com.tom.kafka.examples.KafkaUtils;
import com.tom.kafka.examples.model.StockEvent;
import lombok.extern.slf4j.Slf4j;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.errors.LogAndContinueExceptionHandler;
import org.apache.kafka.streams.kstream.*;
import org.apache.kafka.streams.state.Stores;

import java.util.Properties;
import java.util.concurrent.CountDownLatch;

@Slf4j
public class StockService {

    private CountDownLatch latch = new CountDownLatch(1);

    private StreamsBuilder builder;

    public StockService() {
        super();
        builder = new StreamsBuilder();
    }

    public StockService(StreamsBuilder builder) {
        this.builder = builder;
    }

    public void run() {
        Properties props = new Properties();
        KafkaUtils.addBootstrapServer(props);

        props.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG, LogAndContinueExceptionHandler.class);
        props.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE);
        props.put("enable.auto.commit", "false");
        props.put("isolation.level", "read_committed");
        props.put(StreamsConfig.APPLICATION_ID_CONFIG, StockService.class.getName());

        addTopology();
        final Topology topology = builder.build();
        System.out.println(topology.describe());

        final KafkaStreams streams = new KafkaStreams(topology, props);

        KafkaUtils.addShutdownHook(streams, latch, "StockService");

        start(streams);
    }

    private Long aggregate(String stock, StockEvent stockEvent,
                           Long currentQuantity) {
        log.info("Processing stock event for stock {}", stock);
        long quantitiy = 0l;
        if (stockEvent.getType() == StockEvent.Type.ADDITION) {
            quantitiy = handleAddition(stockEvent, currentQuantity);
        }
        else if (stockEvent.getType() == StockEvent.Type.REMOVAL) {
            quantitiy = handleRemoval(stockEvent, currentQuantity);
        }
        return quantitiy;
    }

    private long handleRemoval(StockEvent stockEvent, Long currentQuantity) {
        long newQuantity = currentQuantity;
        if (currentQuantity - stockEvent.getAmount() < 0) {
            log.info("Insufficient stock of {} for stock {} to remove amount {}", currentQuantity, stockEvent.getStock(), stockEvent.getAmount());
            stockEvent.setStatus(StockEvent.Status.REJECTED);
        }
        else {
            newQuantity = currentQuantity - stockEvent.getAmount();
            stockEvent.setStatus(StockEvent.Status.FULFILLED);
            log.info("Removed {} from stock {}. New quantity is {}", stockEvent.getAmount(), stockEvent.getStock(), newQuantity);
        }
        return newQuantity;
    }

    private long handleAddition(StockEvent stockEvent, Long currentQuantity) {
        stockEvent.setStatus(StockEvent.Status.FULFILLED);
        long newQuantity = currentQuantity + stockEvent.getAmount();
        log.info("Added {} to to stock {}. New quantity is {}", stockEvent.getAmount(), stockEvent.getStock(), newQuantity);
        return newQuantity;
    }

    void addTopology() {
        Initializer<Long> initializer = () -> 0l;

        Aggregator<String, StockEvent, Long> aggregator = (String account, StockEvent stockEvent,
                                                            Long currentQuantity) -> aggregate(account, stockEvent, currentQuantity);

        KStream<String, StockEvent> stream = builder.stream("stock-events", Consumed.with(KafkaUtils.keySerde,
                KafkaUtils.getSerde(StockEvent.class)));

        stream.groupByKey().aggregate(initializer, aggregator, Materialized
                .<String, Long>as(Stores.inMemoryKeyValueStore("stocks")).withCachingDisabled()
                .withKeySerde(Serdes.String()).withValueSerde(Serdes.Long()));
        stream.selectKey((key, stockEvent) -> stockEvent.getId()).to("processed-stocks-events", Produced.with(KafkaUtils.keySerde,
                KafkaUtils.getSerde(StockEvent.class)));
    }

    private void start(KafkaStreams streams) {
        try {
            streams.start();
            latch.await();
        } catch (Throwable e) {
            System.exit(1);
        }
        System.exit(0);
    }

    public static void main(String[] args) {
        StockService stockService = new StockService();
        stockService.run();
    }
}
